\documentclass{article}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{enumerate}
\usepackage{hyperref}
\usepackage{subcaption}
\usepackage{caption}
\usepackage{xcolor}
\usepackage{listings}
\lstset{basicstyle=\ttfamily,
  showstringspaces=false,
  commentstyle=\color{red},
  keywordstyle=\color{blue}
}

\pagestyle{empty} \addtolength{\textwidth}{1.0in}
\addtolength{\textheight}{0.5in}
\addtolength{\oddsidemargin}{-0.5in}
\addtolength{\evensidemargin}{-0.5in}
\newcommand{\ruleskip}{\bigskip\hrule\bigskip}
\newcommand{\nodify}[1]{{\sc #1}}
\newcommand{\points}[1]{{\textbf{[#1 points]}}}
\newcommand{\subquestionpoints}[1]{{[#1 points]}}
\newenvironment{answer}{{\bf Answer:} \sf }{}%

\newcommand{\bitem}{\begin{list}{$\bullet$}%
{\setlength{\itemsep}{0pt}\setlength{\topsep}{0pt}%
\setlength{\rightmargin}{0pt}}}
\newcommand{\eitem}{\end{list}}

\setlength{\parindent}{0pt} \setlength{\parskip}{0.5ex}
\setlength{\unitlength}{1cm}

\newcommand{\pa}[1]{[[PA: #1]]}

\renewcommand{\Re}{{\mathbb R}}
\newcommand{\E}{{\rm E}}
\begin{document}

\pagestyle{myheadings} \markboth{}{CSCI 4302/5302 Advanced Robotics, Homework \#4, Spring 2025}

{\huge
\noindent HW \#4: Value/Policy Iteration, Discretization}\\
\ruleskip
{\bf Name}: Gyanig Kumar

{\bf Deliverable}:  PDF write-up and zip file of project directory.  Your PDF should be generated by replacing the placeholder images in this LaTeX document with the appropriate solution images for each question.  Your PDF and code is to be submitted into the course Canvas. The scripts will automatically generate the appropriate images, so you only need to recompile the LaTeX document to populate it with content. If you do not have/do not want to install LaTeX on your machine, you may use a website like Overleaf instead.

\textbf{Graduate Students:} You are expected to complete the entire assignment.\\
\textbf{Undergraduate Students:} You need only complete questions that do not have \textbf{(GRAD)} next to them. (You do not need to implement max-ent Value Iteration or look-ahead policies.)\\

\vspace{.1in}

You will need to install matplotlib and the Gymnasium Environment for Python:\\
\begin{lstlisting}[language=bash]
pip install gymnasium
pip install matplotlib
\end{lstlisting}
or 
\begin{lstlisting}[language=bash]
conda install gymnasium
conda install matplotlib
\end{lstlisting}

\vspace{.2in}

\section{Gridworld Env}
%--------------------------------------------------------------------------------
%--------------------------------------------------------------------------------
%--------------------------------------------------------------------------------
\subsection{Value Iteration [20pts]}
%--------------------------------------------------------------------------------
%--------------------------------------------------------------------------------
%--------------------------------------------------------------------------------


First, you will implement the value iteration algorithm for the tabular case. You will need to fill the code in \texttt{code/tabular\_solution.py} below the lines \texttt{if self.policy\_type == `deterministic\_vi'}. Run the script for the two gridworld domains and report the heatmap of the converged values.






\newpage
%--------------------------------------------------------------------------------
%--------------------------------------------------------------------------------
%--------------------------------------------------------------------------------
\subsection{Policy Iteration [20 points]}
%--------------------------------------------------------------------------------
%--------------------------------------------------------------------------------
%--------------------------------------------------------------------------------

\begin{enumerate}[(a)]

Next, you will implement the policy iteration algorithm for the tabular case. You will need to fill the code in \texttt{code/tabular\_solution.py} below the lines \texttt{if self.policy\_type == `deterministic\_pi'}. Run the script for the first gridworld domain and turn in a graph with policy value (accumulated reward) on the vertical axis and iteration number on the horizontal axis.



\end{enumerate}


%--------------------------------------------------------------------------------
%--------------------------------------------------------------------------------
%--------------------------------------------------------------------------------
\newpage 
\section{Mountain Car}
%--------------------------------------------------------------------------------
%--------------------------------------------------------------------------------
%--------------------------------------------------------------------------------
\subsection{Near neighbors iterpolation [20pt]}


Value Iteration can only work when the state and action spaces are discrete and finite. If these assumptions do not hold, we can approximate the problem domain by coming up with a discretization such that the previous algorithm is still valid. The \textit{MountainCar} domain, as described in class, has a continuous state space that will prevent us from using value or policy iteration to solve it directly. One of the solutions that we came up with for this was \emph{nearest-neighbor interpolation}, where we discretize the actual state space $S$ into finitely many states $\Xi={\xi_1, \xi_2,...,\xi_n}$, and act as if we are in the $\xi$ nearest to our actual state $s$.

Implement Value Iteration with nearest-neighbor interpolation on the \textit{MountainCar} domain. You will need to add code in \textit{code/continuous\_solution.py} below the lines \textit{if self.\_mode == `nn'}. Run the script and report the state value heatmap for \texttt{MountainCar}, discretizing each dimension of the state space (position and velocity) into 21, 51, and 101 bins.

\newpage
\subsection{Linear interpolation}
Nearest-neighbor interpolation is able to approach the optimal solution if you use a fine-grained approximation, but doesn't scale well as the dimensionality of your problem increases. A more powerful discretization scheme that we discussed in class is \emph{n-linear interpolation}, an $n$-dimensional analogue of linear interpolation. Add your code within \texttt{code/continuous\_solution.py} below the line \texttt{if self.\_mode == `linear'}. Just as before, report the state value heatmap for \texttt{MountainCar} with discretization resolutions of 21, 51, and 101 points per dimension.



\subsection{Stochastic Policy Iteration - Grad Only}
Grad students, implement stochastic policy iteration for the Mountian Car problem. This should be a small update to your existing policy iteration implementation.

\begin{enumerate}[a)]
    \item You should not be considered if your stochastic policy iteration does not work as well as your deterministic implementation, this is expected. Why might this be? Explain your hypothesis.
    \item The idea of  a stationary policy is more difficult to determine in the stochastic case. We have provided an implementation of KL-divergance for you in the codebase, you may use this to determine if your policy is approximately stationary. What is KL-divergance, and why might it be a good way of determining if your policy is stable? If you chose to use something other than KL-divergance, briefly explain.
\end{enumerate}



\end{document}
